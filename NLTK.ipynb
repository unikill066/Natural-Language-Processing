{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLTK.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wI8QlOwW1tL"
      },
      "source": [
        "**NLTK is the mother of all the NLP libraries, and, it is used for building python programs that work with human language data and relavant application(s in statistical natural language processing.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ol81ZvbcmGo"
      },
      "source": [
        "# Importing NLTK\n",
        "import nltk"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sob1APiDdEm9",
        "outputId": "b96b071f-3e2c-4504-c997-d1fa9668e2e7"
      },
      "source": [
        "# Downloaing resource\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8iJ433Ec2mN",
        "outputId": "72616cf3-f737-4ac3-985d-aee60cbeb572"
      },
      "source": [
        "# Lemmatization\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "print(\"happier:\", lemmatizer.lemmatize(\"happier\", pos=\"a\"))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "happier: happy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQu8QY-bfVCr",
        "outputId": "20a2a157-1531-4e18-e909-f0c81afbe91d"
      },
      "source": [
        "# Stemming -- PorterStemmer\n",
        "from nltk.stem import PorterStemmer\n",
        "porter_stemmer = PorterStemmer()\n",
        "print(\"clapping :\",porter_stemmer.stem(\"clapping\"))\n",
        "print(\"clapped :\",porter_stemmer.stem(\"clapped\"))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "clapping : clap\n",
            "clapped : clap\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTgWoSTNfiB3",
        "outputId": "c417209f-c724-45b9-e7bd-80fb87695a81"
      },
      "source": [
        "# Word Normalization\n",
        "sentence = \"Mary had a little Lamb.\"\n",
        "normalized_sentence = sentence.lower()\n",
        "print(normalized_sentence)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mary had a little lamb.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNBEFJDBf4jV",
        "outputId": "dfe6d9ab-72a8-465d-fcf0-36f3791e310f"
      },
      "source": [
        "# Regex\n",
        "import re, string\n",
        "\n",
        "# Replacing all the no's with ' '\n",
        "sentence = \"1000 ships reached the shores of atlantis and invaded the hinterland and established 17 some camps in and around the island.\"\n",
        "words = re.sub(r'\\d+', '', sentence)\n",
        "print(\"REGEX TO SUB ALL THE DIGITS :\", words)\n",
        "\n",
        "# Removing Punctuation\n",
        "sentence = \"Would you like to accompany us in the comming tour? Or, do you have any other plans running in parallel!?\"\n",
        "words = re.sub('[%s]' % re.escape(string.punctuation), '', sentence)\n",
        "print(\"REGEX FOR REMOVING PUNCTUATION :\", words)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "REGEX TO SUB ALL THE DIGITS :  ships reached the shores of atlantis and invaded the hinterland and established  some camps in and around the island.\n",
            "REGEX FOR REMOVING PUNCTUATION : Would you like to accompany us in the comming tour Or do you have any other plans running in parallel\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvOALEWUg_-B",
        "outputId": "bc09d9d3-9205-41e9-8065-6bf44ee73bb6"
      },
      "source": [
        "# Spaces\n",
        "input_str = ' Alt rose to \\n power and \\r\\t\\n \\n\\n went back to the dungens where   \\t he belonged.  '\n",
        "\n",
        "print('Remove spaces using regex :', re.sub(r\"\\s+\", \"\", input_str),\"\\n\", sep='')\n",
        "print('Remove landing spaces using regex :', re.sub(r\"^\\s+\", \"\", input_str),\"\\n\", sep='')\n",
        "print('Remove trailing spaces using regex :', re.sub(r\"\\s+$\", \"\", input_str),\"\\n\", sep='')\n",
        "print('Remove landing spaces using regex :', re.sub(r\"^\\s+|\\s+$\", \"\", input_str),\"\\n\", sep='')\n",
        "\n",
        "# Emails\n",
        "print('EMAIL :', re.sub(r\"^([a-zA-Z0-9_\\-\\.]+)@([a-zA-Z0-9_\\-\\.]+)\\.([a-zA-Z]{2,5})$\", \"\", input_str),\"\\n\", sep='') "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Remove spaces using regex :Altrosetopowerandwentbacktothedungenswherehebelonged.\n",
            "\n",
            "Remove landing spaces using regex :Alt rose to \n",
            " power and \r\t\n",
            " \n",
            "\n",
            " went back to the dungens where   \t he belonged.  \n",
            "\n",
            "Remove trailing spaces using regex : Alt rose to \n",
            " power and \r\t\n",
            " \n",
            "\n",
            " went back to the dungens where   \t he belonged.\n",
            "\n",
            "Remove landing spaces using regex :Alt rose to \n",
            " power and \r\t\n",
            " \n",
            "\n",
            " went back to the dungens where   \t he belonged.\n",
            "\n",
            "EMAIL : Alt rose to \n",
            " power and \r\t\n",
            " \n",
            "\n",
            " went back to the dungens where   \t he belonged.  \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3BqL9A_kp6z",
        "outputId": "c42ffc3a-5e9c-48cb-9668-f76220d27360"
      },
      "source": [
        "# Word Tokenization\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "sentence = \"1000 ships reached the shores of atlantis and invaded the hinterland, and, established 17 some camps in and around the island.\"\n",
        "tokens = word_tokenize(sentence)\n",
        "print(tokens)\n",
        "\n",
        "sent_tokens = sent_tokenize(sentence)\n",
        "print(sent_tokens)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['1000', 'ships', 'reached', 'the', 'shores', 'of', 'atlantis', 'and', 'invaded', 'the', 'hinterland', ',', 'and', ',', 'established', '17', 'some', 'camps', 'in', 'and', 'around', 'the', 'island', '.']\n",
            "['1000 ships reached the shores of atlantis and invaded the hinterland, and, established 17 some camps in and around the island.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksmbracLkQpk",
        "outputId": "834267c6-dbc6-4523-948d-fb3b4e0bc73f"
      },
      "source": [
        "# NLTK Stopwords\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'after',\n",
              " 'again',\n",
              " 'against',\n",
              " 'ain',\n",
              " 'all',\n",
              " 'am',\n",
              " 'an',\n",
              " 'and',\n",
              " 'any',\n",
              " 'are',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'as',\n",
              " 'at',\n",
              " 'be',\n",
              " 'because',\n",
              " 'been',\n",
              " 'before',\n",
              " 'being',\n",
              " 'below',\n",
              " 'between',\n",
              " 'both',\n",
              " 'but',\n",
              " 'by',\n",
              " 'can',\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'd',\n",
              " 'did',\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'do',\n",
              " 'does',\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'doing',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'down',\n",
              " 'during',\n",
              " 'each',\n",
              " 'few',\n",
              " 'for',\n",
              " 'from',\n",
              " 'further',\n",
              " 'had',\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'has',\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'have',\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'having',\n",
              " 'he',\n",
              " 'her',\n",
              " 'here',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'i',\n",
              " 'if',\n",
              " 'in',\n",
              " 'into',\n",
              " 'is',\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'just',\n",
              " 'll',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'me',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'more',\n",
              " 'most',\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'my',\n",
              " 'myself',\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'now',\n",
              " 'o',\n",
              " 'of',\n",
              " 'off',\n",
              " 'on',\n",
              " 'once',\n",
              " 'only',\n",
              " 'or',\n",
              " 'other',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 're',\n",
              " 's',\n",
              " 'same',\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'so',\n",
              " 'some',\n",
              " 'such',\n",
              " 't',\n",
              " 'than',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'the',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'there',\n",
              " 'these',\n",
              " 'they',\n",
              " 'this',\n",
              " 'those',\n",
              " 'through',\n",
              " 'to',\n",
              " 'too',\n",
              " 'under',\n",
              " 'until',\n",
              " 'up',\n",
              " 've',\n",
              " 'very',\n",
              " 'was',\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'we',\n",
              " 'were',\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'what',\n",
              " 'when',\n",
              " 'where',\n",
              " 'which',\n",
              " 'while',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\",\n",
              " 'y',\n",
              " 'you',\n",
              " \"you'd\",\n",
              " \"you'll\",\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves'}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8tpv9X6lPfX",
        "outputId": "c604dfb8-a14e-49d7-d3cb-25ff866a93c9"
      },
      "source": [
        "# Removing the stop words from tokenized sentence\n",
        "filtered_words = [ i for i in tokens if not i in stop_words ]\n",
        "filtered_words"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1000',\n",
              " 'ships',\n",
              " 'reached',\n",
              " 'shores',\n",
              " 'atlantis',\n",
              " 'invaded',\n",
              " 'hinterland',\n",
              " ',',\n",
              " ',',\n",
              " 'established',\n",
              " '17',\n",
              " 'camps',\n",
              " 'around',\n",
              " 'island',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMp7_T1Lm77X"
      },
      "source": [
        "------------------------------------------------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrTdK4gKmwz6",
        "outputId": "51d7c525-f3fb-48b9-8bc6-1693077fe884"
      },
      "source": [
        "# N-GRAM\n",
        "from nltk import ngrams\n",
        "\n",
        "sentence = \"1000 ships reached the shores of atlantis and invaded the hinterland, and, established 17 some camps in and around the island.\"\n",
        "\n",
        "n = 3\n",
        "\n",
        "n_grams = ngrams(sentence.split(), n)\n",
        "for grams in n_grams:\n",
        "  print(grams)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('1000', 'ships', 'reached')\n",
            "('ships', 'reached', 'the')\n",
            "('reached', 'the', 'shores')\n",
            "('the', 'shores', 'of')\n",
            "('shores', 'of', 'atlantis')\n",
            "('of', 'atlantis', 'and')\n",
            "('atlantis', 'and', 'invaded')\n",
            "('and', 'invaded', 'the')\n",
            "('invaded', 'the', 'hinterland,')\n",
            "('the', 'hinterland,', 'and,')\n",
            "('hinterland,', 'and,', 'established')\n",
            "('and,', 'established', '17')\n",
            "('established', '17', 'some')\n",
            "('17', 'some', 'camps')\n",
            "('some', 'camps', 'in')\n",
            "('camps', 'in', 'and')\n",
            "('in', 'and', 'around')\n",
            "('and', 'around', 'the')\n",
            "('around', 'the', 'island.')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hDW43scWwph"
      },
      "source": [
        "**NLTK** being so vasy in functionalities, it does not support word vectors and is SLOW. And, definetely cannot be used for production purpose.\n"
      ]
    }
  ]
}